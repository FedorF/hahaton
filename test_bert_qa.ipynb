{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "colab_type": "code",
    "id": "qcBTXDwCIas8",
    "outputId": "7a322785-d976-4095-8258-5cfe784e1fa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/f9/51824e40f0a23a49eab4fcaa45c1c797cbf9761adedd0b558dab7c958b34/transformers-2.1.1-py3-none-any.whl (311kB)\n",
      "\u001b[K     |████████████████████████████████| 317kB 2.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.18)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
      "\u001b[K     |████████████████████████████████| 860kB 49.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 33.6MB/s \n",
      "\u001b[?25hCollecting regex\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
      "\u001b[K     |████████████████████████████████| 645kB 48.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.18 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.18)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (2.6.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=b41d7123dca67fd985085186ca89c51d4804c8c82e2c69d101569af18aa12cc5\n",
      "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, sentencepiece, regex, transformers\n",
      "Successfully installed regex-2019.11.1 sacremoses-0.0.35 sentencepiece-0.1.83 transformers-2.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "w45A6_4YJRrM",
    "outputId": "10f273d8-7b4b-4418-8896-c8ffb6164b89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9681 sha256=b7bc30ca05c61880f164778f179a76cf1feee485582ddbc15fd46d146c54158b\n",
      "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hZa-DWm3JeEK"
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "zip_file = wget.download('https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XK333OgPJ2ws",
    "outputId": "34101639-f085-4950-cc87-955312340190"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multi_cased_L-12_H-768_A-12.zip'"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3FI3SuBWJ5B1"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "with ZipFile(zip_file, 'r') as zipped:\n",
    "   zipped.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wXS8fBk3KY9Z",
    "outputId": "23eefa35-1c69-4876-f730-f41a45aa9489"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building PyTorch model from configuration: {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_bert:Converting TensorFlow checkpoint from /content/multi_cased_L-12_H-768_A-12/bert_model.ckpt\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/position_embeddings with shape [512, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/word_embeddings with shape [119547, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/pooler/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/pooler/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight cls/predictions/output_bias with shape [119547]\n",
      "INFO:transformers.modeling_bert:Loading TF weight cls/predictions/transform/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight cls/predictions/transform/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight cls/predictions/transform/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight cls/predictions/transform/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight cls/seq_relationship/output_bias with shape [2]\n",
      "INFO:transformers.modeling_bert:Loading TF weight cls/seq_relationship/output_weights with shape [2, 768]\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'predictions', 'output_bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\n",
      "Save PyTorch model to multi_cased_L-12_H-768_A-12/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!transformers bert \\\n",
    "  multi_cased_L-12_H-768_A-12/bert_model.ckpt \\\n",
    "  multi_cased_L-12_H-768_A-12/bert_config.json \\\n",
    "  multi_cased_L-12_H-768_A-12/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I_3sX4PyLE2Z"
   },
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "\n",
    "directory = 'multi_cased_L-12_H-768_A-12/'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "  if 'bert_' == filename[:5]:\n",
    "    shutil.move(directory + '/' + filename, directory + '/' + filename[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "zcSlssGMIniS",
    "outputId": "442a7b2f-d15b-45e9-96dd-c86bf68b9cb6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "pretrained_weights = directory\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "model = BertModel.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eDbU7wx8L_c2"
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# text = requests.get('http://tululu.org/txt.php?id=4876')\n",
    "\n",
    "# https://lenta.ru/news/2019/11/21/mc21/\n",
    "text = \"\"\"\n",
    "Ближне-среднемагистральный самолет МС-21 получит изготовленные из российских композитов панели кессона крыла и центроплана, сообщил в интервью «Интерфакс» индустриальный директор авиационного кластера «Ростеха» Анатолий Сердюков.\n",
    "«Из российских композитов были изготовлены панели кессона крыла и центроплана, которые проходят испытания. Их результаты позитивны. После проведения прочностных испытаний в ЦАГИ [Центральный аэрогидродинамический институт] силовые композитные конструкции для самолета МС-21 будут производиться из российских материалов», — сказал Сердюков.\n",
    "В январе гендиректор «АэроКомпозита» Анатолий Гайданский заявил, что меры, введенные министерством промышленности и торговли США в отношении возглавляемой им компании, выступающей поставщиком композитов для МС-21, являются не санкциями, а «технологическими ограничениями».\n",
    "В октябре 2018 года двигатель ПД-14 получил Сертификат типа с подписью главы Росавиации Александра Нерадько.\n",
    "В январе 2018 года заместитель гендиректора Национального института авиационных технологий, руководитель научно-методического центра «Норма» Василий Подколзин отмечал, что «черное крыло» МС-21 создается по западным технологиям из зарубежных углеволокна и полимерных связующих.\n",
    "Новейший российский ближне-среднемагистральный узкофюзеляжный пассажирский самолет МС-21 («Магистральный самолет XXI века») предназначен для перевозки до 211 человек (предельная плотная компоновка кресел в версии МС-21-300) на расстояние до 6,4 тысячи километров. Первый полет машина совершила в мае 2017-го. В МС-21 используется множество западных технологий и компонентов, в частности, на машину устанавливаются американские двигатели PW1000G, которые планируется заменить российскими ПД-14.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MiqQaQ8TMtYe",
    "outputId": "d1b1d83c-9f7e-4b9d-8acf-3adb9ff752f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1724"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(text.content)\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nEJo_pQuM1Cx",
    "outputId": "67a6fd4a-2f10-41ef-f617-3c1ccbeebd08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentences = text.content.decode('utf-8').split('. ')\n",
    "sentences = [\n",
    "  sentence + '.' for sentence in text.replace('\\n', ' ').split('. ')\n",
    "]\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "OsBI9GfXNPmK",
    "outputId": "b2c6c3d8-a678-4d5b-b948-ac7a509f8c98"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['«Из российских композитов были изготовлены панели кессона крыла и центроплана, которые проходят испытания.',\n",
       " 'Их результаты позитивны.',\n",
       " 'После проведения прочностных испытаний в ЦАГИ [Центральный аэрогидродинамический институт] силовые композитные конструкции для самолета МС-21 будут производиться из российских материалов», — сказал Сердюков.']"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CmKzdzZUNTsh"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IbcdZIMjNbIc",
    "outputId": "45b69d71-c503-45be-cd6a-23930557f1c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:02<00:00,  3.67it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for sentence in tqdm.tqdm(sentences):\n",
    "  tensor = torch.tensor([tokenizer.encode(sentence, add_special_tokens=True)])\n",
    "  with torch.no_grad():\n",
    "        dataset.append(model(tensor)[0].cpu().numpy()[0].mean(axis=0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iRb25tjMOi28",
    "outputId": "cd111793-a413-413a-c089-25e53b80600b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 768)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.array(dataset).reshape(len(dataset), len(dataset[0]))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o1jX7UXpPJRM"
   },
   "outputs": [],
   "source": [
    "database = NearestNeighbors(n_neighbors=1, metric='cosine').fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "mcJ2Q9azPP6K",
    "outputId": "a4eec72a-d7c7-44fa-cc44-fcc10f88fc38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Как называется двигатель?\n",
      "-  В МС-21 используется множество западных технологий и компонентов, в частности, на машину устанавливаются американские двигатели PW1000G, которые планируется заменить российскими ПД-14.\n",
      "---\n",
      "Зачем самолету композитные материалы?\n",
      "-  «Из российских композитов были изготовлены панели кессона крыла и центроплана, которые проходят испытания.\n",
      "---\n",
      "Что сказал директор?\n",
      "-  В январе гендиректор «АэроКомпозита» Анатолий Гайданский заявил, что меры, введенные министерством промышленности и торговли США в отношении возглавляемой им компании, выступающей поставщиком композитов для МС-21, являются не санкциями, а «технологическими ограничениями».\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for question in [\n",
    "  'Как называется двигатель?',\n",
    "  'Зачем самолету композитные материалы?',\n",
    "  'Что сказал директор?'\n",
    "]:\n",
    "  tensor = torch.tensor([tokenizer.encode(question, add_special_tokens=True)])\n",
    "  with torch.no_grad():\n",
    "    vector = model(tensor)[0].cpu().numpy()[0].mean(axis=0)\n",
    "  indices = database.kneighbors([vector], return_distance=False)[0]\n",
    "  print(question)\n",
    "  for index in indices:\n",
    "    print('- ', sentences[index])\n",
    "  print('---')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "test_bert_qa.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
